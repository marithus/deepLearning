{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "939b2bd3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Defining the Problem\n",
    "\n",
    "By general definition, fashion is a popular way of dressing during a particular time. As thechanges made in fashion are very much affected by time, which can refer to seasonal periods(summer, winter, autumn, spring), different fashion eras or even different fashion styles/choices from Retro to Androgynous fashion. The word 'Fashion' we are using here is a broad term which constantly upgrades and changes itself, older styles can come back as the new trend.\n",
    "\n",
    "Data quality and quantity may be a limiting factor when performing an in-depth analysis on all fashion data. Hence, this project would be using a smaller and broader defined set of fashion data that would be more relatable and easily understood by the general population. Although,the data may by limited in quantity and quality, it is much easier understood when looking at it from a broader perspective.\n",
    "\n",
    "### Data Quantity\n",
    "\n",
    "The dataset used may not contain every piece of fashion clothing that exists, the labels are alsolimited where not all types of clothes are identified (limited clothing vocabulary)\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "The dataset used is not identified by seasons/era or other external factors affecting fashion.Different clothings in different periods of time/eras can be labelled differently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0689401f",
   "metadata": {},
   "source": [
    "# Assembling a Dataset\n",
    "\n",
    "In this project, we will be using a tensorflow dataset called fashion MNIST, a dataset differentfrom the ones mentioned in the Coursework Instructions provided (MNIST, Reuters, IMDB andBoston Housing Price).\n",
    "\n",
    "From this dataset, we will be predicting the type of fashion clothing the image belongs to, forexample a label of class 1 refers to 'trouser' and a label of class 8 refers to 'bag' as shownbelow. In addition, the dataset is a multiclass, single-label classification which means that thelast-layer activation would be done using softmax and the loss function will be calculated using'categorical_crossentropy'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset , ds_info = tfds.load('fashion_mnist',split='train',with_info=True) \n",
    "fig=tfds.show_examples(dataset,ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models,layers \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95432e6b",
   "metadata": {},
   "source": [
    "After importing the necessary libraries to be used in the project, we import our dataset usingtensorflow library installed. This gives us the already pre-seperated train and test sets. As wewould require training our models with training and validation sets, we would be further splittingthe dataset into training, validation and test sets in the ratio of 3:1:1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53528495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63af1a3c",
   "metadata": {},
   "source": [
    "This shows the shape of the split dataset of fashion mnist for both images and labels. We would need to pre-process these data to ensure that they can be fed into a machine-learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7492b485",
   "metadata": {},
   "source": [
    "# Baseline 1.1\n",
    "\n",
    "Before preparing the data, we will be using the current format of the data to generate our first baseline.\n",
    "\n",
    "Below we generate the estimated probability of correctly predicting the label. This is done by finding the most frequently occurring label in the training data (by combining the validation and training data we split above) which is 9 (ankle boot), and using this label against the test data to predict the probability of a accurate guess. This gives us the probability where test data is also of label/class 9 which is 2.14%.\n",
    "\n",
    "This baseline is calculated using occurrences in the dataset and depends highly on how the dataset is split, another baseline example of 10% will be further explained below at Baseline 1.2.\n",
    "\n",
    "In addition, the cell below generates the number of samples per label/class showing that there are equal number of samples for each label/class, which we will be using to provide further explaination further on in the report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c4dd28d",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We have to undergo 3 Steps of prepare the data to be used by the models.\n",
    "\n",
    "Step 1.\n",
    "\n",
    "reshape() is called on the images data to flatten the 28 by 28 (2-dimensional) array to a vector of 784 elements.\n",
    "\n",
    "Step 2.\n",
    "The integers are cast as float and re-scaled to lie between 0 and 1.\n",
    "\n",
    "Step 3.\n",
    "\n",
    "The model would require categrically encoded labels where each label will be turned into a 10-element vector of zeros except for a single element. This is called one-hot encoding using to_categorical where at the position in the vector that corresponds to the label will be encoded with a one in the (n+1)th position.\n",
    "\n",
    "This shows the shape of the processed data, when it is ready to be used in the models.\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "For this project we will be using Adam as our optimizer as it is a combination of the best properties of AdaGrad, RMSprop and momentum. Where momentum is \"Momentum takes past gradients into account to smooth out the steps of gradient descent.\" (\"Quick Notes on How to choose Optimizer In Keras | DLology\", 2018)\n",
    "\n",
    "In addition, Adam generally works well in training deep learning models and outperforms other Adaptive techniques. As for our project, as our dataset contains many images similarly to other projects which used Adam to solve optimization problems, I believe Adam would be suited for this project. (Wirth, n.d.)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b1cded",
   "metadata": {},
   "source": [
    "# Loss vs accuracy\n",
    "\n",
    "We have functions that perform the action of plotting of loss and accuracy to be used later in the project when building a model. This is because we would be analysing both graphs to ensure that the model is performing as accurately as possible.\n",
    "\n",
    "We would be using accuracy as our metrics at compilation which will be useful as we have the same amount of samples per label/class (refer to Baseline 1.1 above). This is because accuracy is affected by the number of samples available. However, in this case when we split the data into test, validation and training sets, there is no longer the same amount of samples per label/class. Meaning that accuracy would only show the performace of the classifier at guessing. We should be using 'scoring rules' such as \"loss functions that map predicted probabilities and corresponding observed outcomes to loss values, which are minimized in expectation by the true probabilities (p,1−p)\". \n",
    "\n",
    "Hence, in order to ensure optimal performance of the model. We would plot both accuracy and loss graphs when building and improving on our models to ensure that the model is underfit/overfit/goodfit. (Kolassa, n.d.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb39b351",
   "metadata": {},
   "source": [
    "# Baseline 1.2\n",
    "\n",
    "Before assembling a model we have to define our baseline. What would be the probability of predicting the class without the use of any model?\n",
    "\n",
    "We have found our first example of a baseline above at Baseline 1.1 at 2.14%. Now, we will take a look at a baseline without referencing the dataset but simply by looking at the labels/classes present in the dataset.\n",
    "\n",
    "As there are a total of 10 labels, from classes 0 to 9, for each fashion item. By probability, the percentage of accuracy is 10% for my second baseline where 100%/10 = 10%. As the second baseline of 10% is higher than our first baseline of 2.14%, this project will use the accuracy from the second baseline in order to achieve better results. Hence, I would need to develop a model that is able to do better than 0.1 accuracy in order for it to achieve statistical power.\n",
    "\n",
    "## Evaluation Protocol 1: Hold-Out\n",
    "In most cases, hold out validation is sufficient for our models. So, we will first attempt to create a model using the hold-out method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "825de6db",
   "metadata": {},
   "source": [
    "# Underfit Model\n",
    "The first model would be fairly simple, having the minimum requirements that we need to have statistical power. Which is to have an accuracy higher than 0.1. Our model will only have the required last activation layer softmax and the loss function categorical_crossentropy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fbbc87d",
   "metadata": {},
   "source": [
    "As shown below, our accuracy has a value of 85% despite the underfitting model. This issufficient to beat our baseline of 10%. Now, we will create a model that overfits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32891e0a",
   "metadata": {},
   "source": [
    "# A Model that Overfits\n",
    "\n",
    "After developing a model that is able to do better than the baseline, we need to develop a model that overfits. As the aim of this project is to create a model that stands on the line between overfitting and underfitting, we now need to cross the line to develop a model that overfits. Here, we would be making the model bigger or increasing the training until the model's performance on the validation data starts degrading. This is when overfitting has been achieved. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3274267",
   "metadata": {},
   "source": [
    "From the plotted training and validation loss diagram below, we can tell that validation loss is lowest at around epoch four. From the plotted training and validation accuracy, the validation accuracy first peaks near epoch five. However, the model has been over-trained as it fits the training set very closely and begins to fail on the unseen validation data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a68f4bb6",
   "metadata": {},
   "source": [
    "From the evaluation, accuracy of the overfit model is higher than the underfit model at 89%. However, the plotted graph shows severe overfitting as seen from the large difference between the training and validation loss and accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e22c82ac",
   "metadata": {},
   "source": [
    "# Regularising the Model\n",
    "\n",
    "After creating a model that overfits, we need to modify the model until the line we mentioned above, is found. This can be done using various methods which we will be further exploring in this project.\n",
    "\n",
    "When the model is initialised, we will evaluate the network on the complete test set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d660dd35",
   "metadata": {},
   "source": [
    "# Method 1: DropOut\n",
    "\n",
    "# Method 2: Add L1 and/or L2 regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fab8242d",
   "metadata": {},
   "source": [
    "# Combining the previous few methods\n",
    "\n",
    "From the plotted graphs, we have achieved a model with a good-fit where the training andvalidation curves are close to one another without overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38627bbb",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "Now that we have the final model/network, we will perform the training on the complete training set instead of the partial training set. Hence, we have concatenated both the training and validation images and labels below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cc0015f",
   "metadata": {},
   "source": [
    "In order to prove that our final model works, we will be using various metrices to evaluate the model. \n",
    "<ul>\n",
    "<li>Evaluation on test data: which generates a list containing the combination of the loss and accuracy. </li> \n",
    "<li>Confusion matrix which is a breakdown of predictions into a table showing correct predictions and the types of incorrect predictions made, predictions in the diagonal section of the table are the correct predictions. </li> \n",
    "<li> Precision which measures a classifier's exactness, higher precision means a more accurate classifier </li> \n",
    "<li>Recall which measures a classifier's completeness, higher recall mans more cases the classifier covers. </li> \n",
    "<li>F1 Score is a weighted average of precision and recall. </li> \n",
    "<li> Cohen’s kappa is the classification accuracy normalized by the imbalance of the classes in the data. </li> \n",
    "</ul>\n",
    "(Willems, 2019)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5e0be1a",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "From the evaluation metrics above, we can say that the model is able to perform well and generate moderately accurate results at 87.98%. This is further supported by generating the first 5 values predicted and comparing it against the first 5 values in the test data. As both values generated are the same at \"8, 4, 0, 5, 6\", we are able to perform accurate prediction using the model trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0185013",
   "metadata": {},
   "source": [
    "# Evaluation Protocol 2: Iterated K-fold\n",
    "\n",
    "When deciding an evaluation protocol, there are 3 common methods such as hold-out, K-fold cross and iterated K-fold. Generally the most common method is the Hold-out method which is the method we used above. However, this method of evaluation is highly dependent on the data points in the training and test sets which in turn depends on the method of splitting the data. (Rastogi, 2021)\n",
    "\n",
    "As the dataset contains a total of 70,000 samples and we are performing a deep learning project, the samples can be said to be insufficient. This means our dataset is small which results in an even smaller validation and partial training sets, a small validation set would make the validation scores more sensitive to the constitution of the set which further affects the data points on the split data.\n",
    "\n",
    "From the evaluation made, K-fod cross validation would be an appropriate choice when choosing an evaluation protocol as it reduces the sensitivity of validation of a small set. However, we would be attempting to perform a highly accurate model for this project. Hence, fufilling the requirement of little data available and aiming to perform a highly accurate model evaluation, we would be using iterated K-fold validation as our evaluation protocol. (\"How much data is required for machine learning?\", 2019)\n",
    "\n",
    "Disclaimer: As the flow of both evaluation protocols are similar, we will not be further explaining repeated concepts except comparing the 2 evaluation protocols or when necessary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cea3bbd",
   "metadata": {},
   "source": [
    "As we will be doing a further split of the training set into partial training and validation sets in the iterated K-fold itself, we do not need to split them at the start. This is because the training data will be split into K-portions of equal sizes and trained on the remaining (K-1) partitions, this process will be repeated using different validation sets K number of times. K-fold is iterated multiple times and shufflied each time before splitting it into K-portions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b11aee",
   "metadata": {},
   "source": [
    "Functions to generate a smoother curve and plotting of val_mae graph to be used below \n",
    "\n",
    "When performing iterated K-fold, we will use a mean squared error (MSE) for loss an mean absolute error (MAE) as our monitoring metric. However, as there is no concept of regression accuracy (MSE for regression loss), we would usually monitor training with MAE. We will be plotting graphs only for MAE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00cb394e",
   "metadata": {},
   "source": [
    "## Iterated K-fold on Simple Model\n",
    "\n",
    "First, lets see what happens when we use iterated K-fold on the same underfit model (for hold- out validation) above that performed better than the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe82eb3",
   "metadata": {},
   "source": [
    "From the graphs above, we can tell that there are insufficient layers in our model as lowest MAE cannot occur at epoch 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8d25b6b",
   "metadata": {},
   "source": [
    "# Iterated K-fold on Overfit Model\n",
    "\n",
    "Next, we will perform iterated K-fold validation on the same overfit model (for hold-out validation).\n",
    "\n",
    "\n",
    "From the graph above, the lowest MAE occurs at epoch 27. This means that around 27 epochsis optimal. We can now train the model on the full training set and evaluate our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6a93022",
   "metadata": {},
   "source": [
    "# K Good-Fit\n",
    "\n",
    "We will now train our model on all the training set instead of the partial training sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b37c36d",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "From the evaluation above, we can say that the model is able to generate moderately accurate results as seen from the prediction error on unseen data of 0.0283 x 100 = 2.83%. We can say that the model is able to perform well and generate moderately accurate results based on the low prediction error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d659278",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I have managed to train two different models that were able to perform generally well overall. Both models were built using different evaluation protocols but had a similar network layers where, hold-out validation required regularising the model using drop out and regularisers and iterated k-fold validation required lesser eopchs. Otherwise, the general layout of the model would be as follows: \n",
    "<ul>\n",
    "<li>layers.Dense(512, activation = 'relu',input_shape=(28*28,)) </li>\n",
    "<li>layers.Dense(124, activation = 'relu') </li>\n",
    "<li>layers.Dense(46, activation = 'relu') </li>\n",
    "<li> layers.Dense(10, activation='softmax')</li>\n",
    "</ul>\n",
    "\n",
    "While using adam as the optimizer, categorical_crossentropy as the loss function, the metric is different depending on the evaluation protocol (accuracy vs mae).\n",
    "\n",
    "During the process of building and training the models, tuning of hyperparameters was performed to find the optimal configuration but were not explicitly explained or elaborated. This is because the tuning of hyperparameters required running and generating many cells that were excessive to be included. However, this does not diminish the importance of hyperparameters. \n",
    "\n",
    "In conclusion, although the evaluation protocols used may be different, the general outline of the models are similar as shown above. Although this may be affected by the splitting of data when using hold-out validation and there may also be a more suitable model for iterated k-fold validation. The models are able to generate overall decent results from each of their evaluation.\n",
    "\n",
    "I have managed to achieve 2 different models using 2 different evaluation protocols, where both models that a similar outline. Comapring hold-out validation at 87.98% accuracy and iterated k- fold validation at 2.83% prediction error. It tells us that although both models can perform well, iterated k-fold model might perform better than the hold-out model as hold-out validation can have a prediction error of 100 - 87.98 = 12.02% which is much higher than iterated k-fold. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86bea530",
   "metadata": {},
   "source": [
    "# References\n",
    "Willems, K. (2019). DataCamp. Retrieved 10 February 2022, from \n",
    "https://www.datacamp.com/community/tutorials/deep-learning-python.Rastogi, S. (2021). \n",
    "\n",
    "K-Fold | K-fold Averaging on Deep Learning Classifier. Analytics Vidhya.\n",
    "Retrieved 11 February 2022, from https://www.analyticsvidhya.com/blog/2021/09/how-to-apply-k-fold-averaging-on-deep-learning-classifier/.\n",
    "\n",
    "How much data is required for machine learning?. Quora. (2019). Retrieved 11 February 2022,\n",
    "from https://www.quora.com/How-much-data-is-required-for-machine-learning.\n",
    "\n",
    "Kolassa, S. Why is accuracy not the best measure for assessing classification models?. StackExchange. Retrieved 27 February 2022, from \n",
    "https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models.\n",
    "\n",
    "Wirth, P. Which Optimizer should I use for my ML Project?. Lightly.ai. Retrieved 11 February 2022, from \n",
    "https://www.lightly.ai/post/which-optimizer-should-i-use-for-my-machine-learning-project. \n",
    "\n",
    "Quick Notes on How to choose Optimizer In Keras | DLology. Dlology.com. (2018). Retrieved 12 February 2022, from \n",
    "https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/#disqus_thread.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac7b32039921764e5423c08d86708bfcfc5d65c5ffe1542dfad366340fbbf046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
